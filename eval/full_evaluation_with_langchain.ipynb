{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ TPN-RAG Full Evaluation with LangChain Pipeline\n",
                "\n",
                "This notebook uses the **full production RAG pipeline** including:\n",
                "- ‚úÖ LangChain integration\n",
                "- ‚úÖ Hybrid Search (BM25 + Vector)\n",
                "- ‚úÖ Cross-Encoder Reranking\n",
                "- ‚úÖ Multi-Query Expansion\n",
                "- ‚úÖ All advanced features from `app/services/advanced_rag.py`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "NUM_SAMPLES = 100  # Start with 100, increase to 941 for full eval\n",
                "CHECKPOINT_EVERY = 10\n",
                "OUTPUT_FILE = \"full_eval_langchain_results.json\"\n",
                "\n",
                "# RAG Configuration (Advanced Features)\n",
                "RAG_CONFIG = {\n",
                "    'top_k': 10,                        # Retrieval depth\n",
                "    'enable_hybrid': True,              # BM25 + Vector\n",
                "    'enable_reranking': True,           # Cross-encoder reranking\n",
                "    'enable_multi_query': False,        # Query expansion (slower but better)\n",
                "    'reranker_model': 'BAAI/bge-reranker-v2-m3',\n",
                "    'initial_k': 20,                    # Retrieve 20, rerank to top_k\n",
                "}\n",
                "\n",
                "# Model configuration  \n",
                "LLM_MODEL = \"chandramax/tpn-gpt-oss-20b\"\n",
                "JUDGE_MODEL = \"gpt-5-mini\"\n",
                "\n",
                "print(f\"üìä Will evaluate {NUM_SAMPLES} samples with advanced RAG\")\n",
                "print(f\"   Hybrid Search: {RAG_CONFIG['enable_hybrid']}\")\n",
                "print(f\"   Reranking: {RAG_CONFIG['enable_reranking']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup paths and imports\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "print(f\"‚úÖ Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load LangChain RAG Pipeline (ONCE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the production RAG pipeline\n",
                "from app.rag_pipeline import TPN_RAG, PipelineConfig, PipelineMode\n",
                "\n",
                "print(\"üîÑ Initializing TPN_RAG pipeline...\")\n",
                "start = time.time()\n",
                "\n",
                "# Create production config\n",
                "config = PipelineConfig(\n",
                "    llm_model=\"qwen2.5:7b\",  # For testing; switch to \"chandramax/tpn-gpt-oss-20b\" for full eval\n",
                "    embed_model=\"qwen3-embedding:0.6b\",\n",
                "    mode=PipelineMode.STANDARD,\n",
                "    top_k=RAG_CONFIG['top_k'],\n",
                "    require_grounding=True\n",
                ")\n",
                "\n",
                "# Initialize RAG pipeline\n",
                "rag = TPN_RAG(config=config)\n",
                "rag.initialize()\n",
                "\n",
                "print(f\"‚úÖ RAG pipeline initialized in {time.time()-start:.1f}s\")\n",
                "print(f\"   Stats: {rag.get_stats()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OR: Load the fine-tuned 20B model directly for more control\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "USE_FINETUNED_MODEL = True  # Set to True to use chandramax/tpn-gpt-oss-20b\n",
                "\n",
                "if USE_FINETUNED_MODEL:\n",
                "    print(f\"üîÑ Loading fine-tuned LLM: {LLM_MODEL}...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
                "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "        LLM_MODEL,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "    \n",
                "    print(f\"‚úÖ Fine-tuned LLM loaded in {time.time()-start:.1f}s\")\n",
                "    print(f\"   Device: {next(llm_model.parameters()).device}\")\n",
                "    print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚è© Using RAG pipeline's default LLM\")\n",
                "    llm_model = None\n",
                "    tokenizer = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Load Advanced RAG Components (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load advanced RAG components if available\n",
                "try:\n",
                "    from app.services.advanced_rag import AdvancedRAGConfig, AdvancedRAGPipeline\n",
                "    \n",
                "    advanced_config = AdvancedRAGConfig(\n",
                "        enable_bm25_hybrid=RAG_CONFIG['enable_hybrid'],\n",
                "        enable_cross_encoder_rerank=RAG_CONFIG['enable_reranking'],\n",
                "        reranker_model=RAG_CONFIG['reranker_model'],\n",
                "        initial_k=RAG_CONFIG['initial_k'],\n",
                "        rerank_top_k=RAG_CONFIG['top_k'],\n",
                "        enable_multi_query=RAG_CONFIG['enable_multi_query'],\n",
                "        enable_rrf=True,\n",
                "        rrf_k=60\n",
                "    )\n",
                "    \n",
                "    print(\"‚úÖ Advanced RAG config loaded\")\n",
                "    print(f\"   Hybrid Search: {advanced_config.enable_bm25_hybrid}\")\n",
                "    print(f\"   Reranking: {advanced_config.enable_cross_encoder_rerank}\")\n",
                "    HAS_ADVANCED_RAG = True\n",
                "    \n",
                "except ImportError as e:\n",
                "    print(f\"‚ö†Ô∏è Advanced RAG not available: {e}\")\n",
                "    print(\"   Using basic vector search only\")\n",
                "    HAS_ADVANCED_RAG = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Load Test Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "test_file = project_root / \"eval\" / \"data\" / \"test_with_citations.jsonl\"\n",
                "\n",
                "samples = []\n",
                "with open(test_file, 'r') as f:\n",
                "    for line in f:\n",
                "        samples.append(json.loads(line))\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(samples)} test samples\")\n",
                "print(f\"üìä Will evaluate first {min(NUM_SAMPLES, len(samples))} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_qa(sample: dict) -> Tuple[str, str]:\n",
                "    \"\"\"Extract question and expected answer from sample.\"\"\"\n",
                "    question = None\n",
                "    expected = None\n",
                "    for msg in sample['messages']:\n",
                "        if msg['role'] == 'user':\n",
                "            question = msg['content']\n",
                "        elif msg['role'] == 'assistant':\n",
                "            expected = msg.get('content', '')\n",
                "    return question, expected\n",
                "\n",
                "# Test extraction\n",
                "q, a = extract_qa(samples[0])\n",
                "print(f\"Sample Q: {q[:100]}...\")\n",
                "print(f\"Sample A: {a[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Inference Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_rag_query(question: str, use_rag: bool = True) -> Tuple[str, List[str], float]:\n",
                "    \"\"\"\n",
                "    Run query through the LangChain RAG pipeline.\n",
                "    \n",
                "    Returns:\n",
                "        answer: Generated answer\n",
                "        context_list: Retrieved documents (for faithfulness eval)\n",
                "        retrieval_time: Time taken for retrieval\n",
                "    \"\"\"\n",
                "    start = time.time()\n",
                "    \n",
                "    if use_rag:\n",
                "        # Use the full RAG pipeline\n",
                "        result = rag.ask(\n",
                "            question=question,\n",
                "            answer_type=\"single\",\n",
                "            require_grounding=True\n",
                "        )\n",
                "        \n",
                "        answer = result.answer\n",
                "        context_list = [result.context_used] if result.context_used else []\n",
                "        \n",
                "        # Get individual sources for faithfulness eval\n",
                "        if hasattr(result, 'sources') and result.sources:\n",
                "            context_list = [s.get('content', '') for s in result.sources if 'content' in s]\n",
                "    else:\n",
                "        # No RAG - just use the LLM directly\n",
                "        if llm_model is not None:\n",
                "            answer = run_direct_inference(question)\n",
                "        else:\n",
                "            # Use RAG pipeline but with empty context\n",
                "            result = rag.ask(question=question, require_grounding=False)\n",
                "            answer = result.answer\n",
                "        context_list = []\n",
                "    \n",
                "    return answer, context_list, time.time() - start\n",
                "\n",
                "\n",
                "def run_direct_inference(question: str, max_tokens: int = 1024) -> str:\n",
                "    \"\"\"Run the fine-tuned model directly without RAG context.\"\"\"\n",
                "    if llm_model is None:\n",
                "        raise ValueError(\"Fine-tuned model not loaded\")\n",
                "    \n",
                "    system_prompt = \"\"\"You are a clinical expert in neonatal/pediatric TPN. \n",
                "Answer accurately based on your training.\"\"\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"developer\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}\n",
                "    ]\n",
                "    \n",
                "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = llm_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            temperature=0.1,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
                "    return response.strip()\n",
                "\n",
                "print(\"‚úÖ Inference functions ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Metric Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DeepEval metrics\n",
                "from deepeval.metrics import GEval, FaithfulnessMetric, AnswerRelevancyMetric, ContextualRecallMetric\n",
                "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
                "\n",
                "# Clinical GEval\n",
                "clinical_geval = GEval(\n",
                "    name=\"Clinical Correctness\",\n",
                "    criteria=\"\"\"Evaluate clinical correctness for TPN:\n",
                "1. DOSING VALUES: Are numeric values exactly correct?\n",
                "2. UNITS: Are units correct? (mg vs g, mEq vs mmol)\n",
                "3. RANGES: Are ranges accurate?\n",
                "4. POPULATIONS: Is patient population correct?\n",
                "5. SAFETY: Are contraindications mentioned?\"\"\",\n",
                "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
                "    model=JUDGE_MODEL,\n",
                "    threshold=0.7\n",
                ")\n",
                "\n",
                "faithfulness = FaithfulnessMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "relevancy = AnswerRelevancyMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "recall = ContextualRecallMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "\n",
                "print(\"‚úÖ DeepEval metrics ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Run Full Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize or resume\n",
                "results = []\n",
                "start_idx = 0\n",
                "\n",
                "checkpoint_file = Path(OUTPUT_FILE.replace('.json', '_checkpoint.json'))\n",
                "if checkpoint_file.exists():\n",
                "    with open(checkpoint_file, 'r') as f:\n",
                "        checkpoint = json.load(f)\n",
                "        results = checkpoint['results']\n",
                "        start_idx = len(results)\n",
                "        print(f\"üìÇ Resuming from checkpoint: {start_idx} samples done\")\n",
                "else:\n",
                "    print(f\"üÜï Starting fresh evaluation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MAIN EVALUATION LOOP\n",
                "num_to_eval = min(NUM_SAMPLES, len(samples))\n",
                "\n",
                "for idx in tqdm(range(start_idx, num_to_eval), desc=\"Evaluating\"):\n",
                "    sample = samples[idx]\n",
                "    question, expected = extract_qa(sample)\n",
                "    \n",
                "    try:\n",
                "        # Phase 1: No RAG\n",
                "        if llm_model is not None:\n",
                "            phase1_answer = run_direct_inference(question)\n",
                "            phase1_context = []\n",
                "        else:\n",
                "            phase1_answer, phase1_context, _ = run_rag_query(question, use_rag=False)\n",
                "        \n",
                "        # Phase 2: With RAG (using LangChain pipeline)\n",
                "        phase2_answer, phase2_context, retrieval_time = run_rag_query(question, use_rag=True)\n",
                "        \n",
                "        # Evaluate Phase 1\n",
                "        tc1 = LLMTestCase(input=question, actual_output=phase1_answer, expected_output=expected)\n",
                "        clinical_geval.measure(tc1)\n",
                "        p1_clinical = clinical_geval.score\n",
                "        \n",
                "        # Evaluate Phase 2\n",
                "        tc2 = LLMTestCase(\n",
                "            input=question, \n",
                "            actual_output=phase2_answer, \n",
                "            expected_output=expected,\n",
                "            retrieval_context=phase2_context if phase2_context else [\"No context retrieved\"]\n",
                "        )\n",
                "        clinical_geval.measure(tc2)\n",
                "        p2_clinical = clinical_geval.score\n",
                "        \n",
                "        # Faithfulness & Recall (Phase 2 only)\n",
                "        if phase2_context:\n",
                "            faithfulness.measure(tc2)\n",
                "            faith_score = faithfulness.score\n",
                "            \n",
                "            recall.measure(tc2)\n",
                "            recall_score = recall.score\n",
                "        else:\n",
                "            faith_score = 0.0\n",
                "            recall_score = 0.0\n",
                "        \n",
                "        # Relevancy\n",
                "        relevancy.measure(tc2)\n",
                "        rel_score = relevancy.score\n",
                "        \n",
                "        result = {\n",
                "            'idx': idx,\n",
                "            'question': question[:150],\n",
                "            'p1_clinical': p1_clinical,\n",
                "            'p2_clinical': p2_clinical,\n",
                "            'rag_lift': p2_clinical - p1_clinical,\n",
                "            'faithfulness': faith_score,\n",
                "            'relevancy': rel_score,\n",
                "            'ctx_recall': recall_score,\n",
                "            'retrieval_time': retrieval_time,\n",
                "            'num_docs_retrieved': len(phase2_context)\n",
                "        }\n",
                "        results.append(result)\n",
                "        \n",
                "        # Checkpoint\n",
                "        if (idx + 1) % CHECKPOINT_EVERY == 0:\n",
                "            with open(checkpoint_file, 'w') as f:\n",
                "                json.dump({'results': results}, f)\n",
                "            print(f\"  üíæ Checkpoint at {idx+1}\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Error on sample {idx}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        continue\n",
                "\n",
                "print(f\"\\n‚úÖ Evaluation complete: {len(results)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(f\"üìä FULL EVALUATION RESULTS (n={len(df)})\")\n",
                "print(\"=\"*70)\n",
                "print()\n",
                "\n",
                "print(\"CLINICAL CORRECTNESS:\")\n",
                "print(f\"  Phase 1 (No RAG):   {df['p1_clinical'].mean():.1%} (std: {df['p1_clinical'].std():.1%})\")\n",
                "print(f\"  Phase 2 (With RAG): {df['p2_clinical'].mean():.1%} (std: {df['p2_clinical'].std():.1%})\")\n",
                "print(f\"  RAG Lift:           +{df['rag_lift'].mean():.1%}\")\n",
                "print(f\"  RAG Lift %:         +{(df['p2_clinical'].mean() - df['p1_clinical'].mean()) / df['p1_clinical'].mean() * 100:.1f}%\")\n",
                "print()\n",
                "\n",
                "print(\"RAG QUALITY METRICS:\")\n",
                "print(f\"  Faithfulness:       {df['faithfulness'].mean():.1%}\")\n",
                "print(f\"  Answer Relevancy:   {df['relevancy'].mean():.1%}\")\n",
                "print(f\"  Contextual Recall:  {df['ctx_recall'].mean():.1%}\")\n",
                "print()\n",
                "\n",
                "print(\"SCORE DISTRIBUTION (Phase 2):\")\n",
                "print(f\"  Perfect (100%):  {(df['p2_clinical'] >= 1.0).sum()} ({(df['p2_clinical'] >= 1.0).mean():.0%})\")\n",
                "print(f\"  Passing (‚â•70%):  {(df['p2_clinical'] >= 0.7).sum()} ({(df['p2_clinical'] >= 0.7).mean():.0%})\")\n",
                "print(f\"  Failing (<50%):  {(df['p2_clinical'] < 0.5).sum()} ({(df['p2_clinical'] < 0.5).mean():.0%})\")\n",
                "print()\n",
                "\n",
                "print(\"PERFORMANCE:\")\n",
                "print(f\"  Avg retrieval time: {df['retrieval_time'].mean():.2f}s\")\n",
                "print(f\"  Avg docs retrieved: {df['num_docs_retrieved'].mean():.1f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. Clinical Correctness Distribution\n",
                "ax = axes[0, 0]\n",
                "ax.hist(df['p1_clinical'], bins=10, alpha=0.5, label='No RAG', color='red')\n",
                "ax.hist(df['p2_clinical'], bins=10, alpha=0.5, label='With RAG', color='green')\n",
                "ax.set_xlabel('Clinical Correctness')\n",
                "ax.set_ylabel('Count')\n",
                "ax.set_title('Score Distribution Comparison')\n",
                "ax.legend()\n",
                "\n",
                "# 2. RAG Lift by Sample\n",
                "ax = axes[0, 1]\n",
                "colors = ['green' if x > 0 else 'red' for x in df['rag_lift']]\n",
                "ax.bar(range(len(df)), df['rag_lift'], color=colors, alpha=0.7)\n",
                "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
                "ax.axhline(y=df['rag_lift'].mean(), color='blue', linestyle='--', label=f'Mean: +{df[\"rag_lift\"].mean():.1%}')\n",
                "ax.set_xlabel('Sample Index')\n",
                "ax.set_ylabel('RAG Lift')\n",
                "ax.set_title('RAG Improvement by Sample')\n",
                "ax.legend()\n",
                "\n",
                "# 3. Key Metrics\n",
                "ax = axes[1, 0]\n",
                "metrics = ['Clinical\\nCorrectness', 'Faithfulness', 'Relevancy', 'Recall']\n",
                "values = [df['p2_clinical'].mean(), df['faithfulness'].mean(), df['relevancy'].mean(), df['ctx_recall'].mean()]\n",
                "colors = ['#2ecc71' if v >= 0.8 else '#f39c12' if v >= 0.6 else '#e74c3c' for v in values]\n",
                "bars = ax.bar(metrics, values, color=colors)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.axhline(y=0.7, color='gray', linestyle='--', alpha=0.5, label='Threshold (70%)')\n",
                "ax.set_title('RAG Quality Metrics')\n",
                "ax.legend()\n",
                "for bar, v in zip(bars, values):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, v + 0.02, f'{v:.0%}', ha='center', fontweight='bold')\n",
                "\n",
                "# 4. Summary Box\n",
                "ax = axes[1, 1]\n",
                "ax.axis('off')\n",
                "summary = f\"\"\"\n",
                "EVALUATION SUMMARY\n",
                "{'='*40}\n",
                "\n",
                "Total Samples:     {len(df)}\n",
                "RAG Config:        top_k={RAG_CONFIG['top_k']}, hybrid={RAG_CONFIG['enable_hybrid']}\n",
                "\n",
                "RESULTS:\n",
                "  Clinical (No RAG):   {df['p1_clinical'].mean():.1%}\n",
                "  Clinical (With RAG): {df['p2_clinical'].mean():.1%}\n",
                "  RAG Lift:            +{(df['p2_clinical'].mean() - df['p1_clinical'].mean()) / df['p1_clinical'].mean() * 100:.1f}%\n",
                "\n",
                "  Faithfulness:        {df['faithfulness'].mean():.1%}\n",
                "  Contextual Recall:   {df['ctx_recall'].mean():.1%}\n",
                "\n",
                "PASS RATE:\n",
                "  ‚â•70%: {(df['p2_clinical'] >= 0.7).mean():.0%} ({(df['p2_clinical'] >= 0.7).sum()}/{len(df)})\n",
                "\"\"\"\n",
                "ax.text(0.1, 0.95, summary, transform=ax.transAxes, fontsize=11,\n",
                "        verticalalignment='top', fontfamily='monospace',\n",
                "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eval_results_langchain.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final results\n",
                "final = {\n",
                "    'num_samples': len(results),\n",
                "    'config': RAG_CONFIG,\n",
                "    'summary': {\n",
                "        'p1_clinical': df['p1_clinical'].mean(),\n",
                "        'p2_clinical': df['p2_clinical'].mean(),\n",
                "        'rag_lift': df['rag_lift'].mean(),\n",
                "        'rag_lift_pct': (df['p2_clinical'].mean() - df['p1_clinical'].mean()) / df['p1_clinical'].mean() * 100,\n",
                "        'faithfulness': df['faithfulness'].mean(),\n",
                "        'relevancy': df['relevancy'].mean(),\n",
                "        'ctx_recall': df['ctx_recall'].mean(),\n",
                "        'pass_rate': (df['p2_clinical'] >= 0.7).mean(),\n",
                "        'perfect_rate': (df['p2_clinical'] >= 1.0).mean()\n",
                "    },\n",
                "    'results': results\n",
                "}\n",
                "\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    json.dump(final, f, indent=2)\n",
                "\n",
                "print(f\"‚úÖ Saved to {OUTPUT_FILE}\")\n",
                "\n",
                "# Cleanup checkpoint\n",
                "if checkpoint_file.exists():\n",
                "    checkpoint_file.unlink()\n",
                "    print(\"üóëÔ∏è Checkpoint removed\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}