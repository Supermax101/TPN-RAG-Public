{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¬ TPN-RAG Full Evaluation Notebook\n",
                "\n",
                "## Advantages of Notebook Evaluation:\n",
                "- âœ… **Load models ONCE** - keep in GPU memory\n",
                "- âœ… **Run incrementally** - test 10 samples, then 100, then all\n",
                "- âœ… **Checkpoint progress** - save every N samples\n",
                "- âœ… **Interactive analysis** - explore results in real-time\n",
                "- âœ… **Resume on failure** - don't lose progress\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - EDIT THESE\n",
                "NUM_SAMPLES = 100  # Start with 100, increase to 941 for full eval\n",
                "TOP_K = 10  # Retrieval depth (5 was too low)\n",
                "CHECKPOINT_EVERY = 10  # Save results every N samples\n",
                "OUTPUT_FILE = \"full_eval_results.json\"\n",
                "\n",
                "# Model configuration\n",
                "LLM_MODEL = \"chandramax/tpn-gpt-oss-20b\"\n",
                "EMBED_MODEL = \"Qwen/Qwen3-Embedding-8B\"\n",
                "JUDGE_MODEL = \"gpt-5-mini\"  # For DeepEval metrics\n",
                "\n",
                "print(f\"ðŸ“Š Will evaluate {NUM_SAMPLES} samples with top_k={TOP_K}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import json\n",
                "import os\n",
                "import sys\n",
                "import time\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from dataclasses import dataclass, asdict\n",
                "import re\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Set environment\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "\n",
                "# Add project root\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "print(f\"âœ… Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Load Models ONCE (Keep in Memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load LLM Model (20B) - ONLY RUN ONCE!\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "print(f\"ðŸ”„ Loading LLM: {LLM_MODEL}...\")\n",
                "start = time.time()\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    LLM_MODEL,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(f\"âœ… LLM loaded in {time.time()-start:.1f}s\")\n",
                "print(f\"   Device: {next(model.parameters()).device}\")\n",
                "print(f\"   Memory: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Embedding Model - ONLY RUN ONCE!\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "print(f\"ðŸ”„ Loading embedding model: {EMBED_MODEL}...\")\n",
                "start = time.time()\n",
                "\n",
                "embed_model = SentenceTransformer(\n",
                "    EMBED_MODEL,\n",
                "    trust_remote_code=True,\n",
                "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
                ")\n",
                "\n",
                "print(f\"âœ… Embedding model loaded in {time.time()-start:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ChromaDB - ONLY RUN ONCE!\n",
                "import chromadb\n",
                "\n",
                "chroma_path = project_root / \"data\" / \"chroma\"\n",
                "client = chromadb.PersistentClient(path=str(chroma_path))\n",
                "collection = client.get_collection(\"tpn_documents\")\n",
                "\n",
                "print(f\"âœ… ChromaDB loaded: {collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Load Test Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "test_file = project_root / \"eval\" / \"data\" / \"test_with_citations.jsonl\"\n",
                "\n",
                "samples = []\n",
                "with open(test_file, 'r') as f:\n",
                "    for line in f:\n",
                "        samples.append(json.loads(line))\n",
                "\n",
                "print(f\"âœ… Loaded {len(samples)} test samples\")\n",
                "print(f\"ðŸ“Š Will evaluate first {min(NUM_SAMPLES, len(samples))} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview first sample\n",
                "sample = samples[0]\n",
                "for msg in sample['messages']:\n",
                "    print(f\"[{msg['role']}]: {msg['content'][:200]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_qa(sample: dict) -> Tuple[str, str]:\n",
                "    \"\"\"Extract question and expected answer from sample.\"\"\"\n",
                "    question = None\n",
                "    expected = None\n",
                "    for msg in sample['messages']:\n",
                "        if msg['role'] == 'user':\n",
                "            question = msg['content']\n",
                "        elif msg['role'] == 'assistant':\n",
                "            expected = msg.get('content', '')\n",
                "    return question, expected\n",
                "\n",
                "\n",
                "def retrieve_context(question: str, top_k: int = TOP_K) -> Tuple[str, List[str], List[Dict]]:\n",
                "    \"\"\"Retrieve relevant context from ChromaDB.\"\"\"\n",
                "    query_embedding = embed_model.encode([question], prompt_name=\"query\")[0].tolist()\n",
                "    \n",
                "    results = collection.query(\n",
                "        query_embeddings=[query_embedding],\n",
                "        n_results=top_k,\n",
                "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
                "    )\n",
                "    \n",
                "    context_parts = []\n",
                "    context_list = []\n",
                "    sources = []\n",
                "    \n",
                "    for i in range(len(results['documents'][0])):\n",
                "        doc = results['documents'][0][i]\n",
                "        meta = results['metadatas'][0][i] if results['metadatas'] else {}\n",
                "        distance = results['distances'][0][i] if results['distances'] else 0\n",
                "        \n",
                "        source_name = meta.get('source', meta.get('document_name', 'Unknown'))\n",
                "        score = 1 - distance\n",
                "        \n",
                "        context_parts.append(f\"[Source: {source_name}]\\n{doc}\")\n",
                "        context_list.append(doc)\n",
                "        sources.append({'source': source_name, 'score': score})\n",
                "    \n",
                "    return \"\\n\\n---\\n\\n\".join(context_parts), context_list, sources\n",
                "\n",
                "\n",
                "def run_inference(question: str, context: Optional[str] = None, max_tokens: int = 1024) -> str:\n",
                "    \"\"\"Run model inference with or without context.\"\"\"\n",
                "    if context:\n",
                "        system_prompt = f\"\"\"You are a clinical expert in neonatal/pediatric TPN.\n",
                "Use these reference documents to answer accurately with citations:\n",
                "\n",
                "<reference_documents>\n",
                "{context}\n",
                "</reference_documents>\n",
                "\n",
                "Always cite sources as [Source Name: \"quote\"].\"\"\"\n",
                "    else:\n",
                "        system_prompt = \"You are a clinical expert in neonatal/pediatric TPN. Answer accurately.\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"developer\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": question}\n",
                "    ]\n",
                "    \n",
                "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            temperature=0.1,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
                "    return response.strip()\n",
                "\n",
                "print(\"âœ… Helper functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Metric Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deterministic metrics (fast, no API calls)\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
                "from rouge_score import rouge_scorer\n",
                "import nltk\n",
                "\n",
                "# Download NLTK data\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
                "smoother = SmoothingFunction().method1\n",
                "\n",
                "\n",
                "def compute_deterministic(reference: str, hypothesis: str) -> Dict:\n",
                "    \"\"\"Compute BLEU, ROUGE, F1 (fast, no API).\"\"\"\n",
                "    ref_tokens = word_tokenize(reference.lower())\n",
                "    hyp_tokens = word_tokenize(hypothesis.lower())\n",
                "    \n",
                "    if len(hyp_tokens) == 0:\n",
                "        return {'bleu_1': 0, 'bleu_4': 0, 'rouge_l': 0, 'f1': 0}\n",
                "    \n",
                "    bleu_1 = sentence_bleu([ref_tokens], hyp_tokens, weights=(1,0,0,0), smoothing_function=smoother)\n",
                "    bleu_4 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoother)\n",
                "    rouge_l = rouge.score(reference, hypothesis)['rougeL'].fmeasure\n",
                "    \n",
                "    # F1\n",
                "    ref_set = set(ref_tokens)\n",
                "    hyp_set = set(hyp_tokens)\n",
                "    common = ref_set & hyp_set\n",
                "    precision = len(common) / len(hyp_set) if hyp_set else 0\n",
                "    recall = len(common) / len(ref_set) if ref_set else 0\n",
                "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    return {'bleu_1': bleu_1, 'bleu_4': bleu_4, 'rouge_l': rouge_l, 'f1': f1}\n",
                "\n",
                "print(\"âœ… Deterministic metrics ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LLM-based metrics (DeepEval - uses API)\n",
                "from deepeval.metrics import GEval, FaithfulnessMetric, AnswerRelevancyMetric, ContextualRecallMetric\n",
                "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
                "\n",
                "# Clinical correctness GEval\n",
                "clinical_geval = GEval(\n",
                "    name=\"Clinical Correctness\",\n",
                "    criteria=\"\"\"Evaluate clinical correctness for TPN with emphasis on:\n",
                "1. DOSING VALUES: Are numeric values exactly correct?\n",
                "2. UNITS: Are units correct? (mg vs g, mEq vs mmol)\n",
                "3. RANGES: Are ranges accurate?\n",
                "4. POPULATIONS: Is patient population correct?\n",
                "5. SAFETY: Are contraindications mentioned?\"\"\",\n",
                "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
                "    model=JUDGE_MODEL,\n",
                "    threshold=0.7\n",
                ")\n",
                "\n",
                "faithfulness_metric = FaithfulnessMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "recall_metric = ContextualRecallMetric(threshold=0.7, model=JUDGE_MODEL)\n",
                "\n",
                "print(\"âœ… DeepEval metrics ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6ï¸âƒ£ Run Evaluation (with Checkpointing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize results\n",
                "results = []\n",
                "start_idx = 0\n",
                "\n",
                "# Check for existing checkpoint\n",
                "checkpoint_file = Path(OUTPUT_FILE.replace('.json', '_checkpoint.json'))\n",
                "if checkpoint_file.exists():\n",
                "    with open(checkpoint_file, 'r') as f:\n",
                "        checkpoint = json.load(f)\n",
                "        results = checkpoint['results']\n",
                "        start_idx = len(results)\n",
                "        print(f\"ðŸ“‚ Resuming from checkpoint: {start_idx} samples already done\")\n",
                "else:\n",
                "    print(f\"ðŸ†• Starting fresh evaluation\")\n",
                "\n",
                "print(f\"ðŸ“Š Will evaluate samples {start_idx} to {min(NUM_SAMPLES, len(samples))-1}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MAIN EVALUATION LOOP\n",
                "num_to_eval = min(NUM_SAMPLES, len(samples))\n",
                "\n",
                "for idx in tqdm(range(start_idx, num_to_eval), desc=\"Evaluating\"):\n",
                "    sample = samples[idx]\n",
                "    question, expected = extract_qa(sample)\n",
                "    \n",
                "    try:\n",
                "        # Phase 1: Model only (no RAG)\n",
                "        phase1_answer = run_inference(question, context=None)\n",
                "        \n",
                "        # Retrieve context\n",
                "        context, context_list, sources = retrieve_context(question, top_k=TOP_K)\n",
                "        \n",
                "        # Phase 2: Model + RAG\n",
                "        phase2_answer = run_inference(question, context=context)\n",
                "        \n",
                "        # Deterministic metrics (fast)\n",
                "        p1_det = compute_deterministic(expected, phase1_answer)\n",
                "        p2_det = compute_deterministic(expected, phase2_answer)\n",
                "        \n",
                "        # LLM metrics (slower, API calls)\n",
                "        # Phase 1 clinical\n",
                "        tc1 = LLMTestCase(input=question, actual_output=phase1_answer, expected_output=expected)\n",
                "        clinical_geval.measure(tc1)\n",
                "        p1_clinical = clinical_geval.score\n",
                "        \n",
                "        # Phase 2 clinical\n",
                "        tc2 = LLMTestCase(input=question, actual_output=phase2_answer, expected_output=expected, retrieval_context=context_list)\n",
                "        clinical_geval.measure(tc2)\n",
                "        p2_clinical = clinical_geval.score\n",
                "        \n",
                "        # Faithfulness (Phase 2 only)\n",
                "        faithfulness_metric.measure(tc2)\n",
                "        faithfulness = faithfulness_metric.score\n",
                "        \n",
                "        # Answer Relevancy\n",
                "        relevancy_metric.measure(tc2)\n",
                "        relevancy = relevancy_metric.score\n",
                "        \n",
                "        # Contextual Recall\n",
                "        recall_metric.measure(tc2)\n",
                "        ctx_recall = recall_metric.score\n",
                "        \n",
                "        # Store result\n",
                "        result = {\n",
                "            'idx': idx,\n",
                "            'question': question[:200],\n",
                "            'p1_clinical': p1_clinical,\n",
                "            'p2_clinical': p2_clinical,\n",
                "            'rag_lift': p2_clinical - p1_clinical,\n",
                "            'faithfulness': faithfulness,\n",
                "            'relevancy': relevancy,\n",
                "            'ctx_recall': ctx_recall,\n",
                "            'p1_bleu4': p1_det['bleu_4'],\n",
                "            'p2_bleu4': p2_det['bleu_4'],\n",
                "            'p1_f1': p1_det['f1'],\n",
                "            'p2_f1': p2_det['f1']\n",
                "        }\n",
                "        results.append(result)\n",
                "        \n",
                "        # Checkpoint every N samples\n",
                "        if (idx + 1) % CHECKPOINT_EVERY == 0:\n",
                "            with open(checkpoint_file, 'w') as f:\n",
                "                json.dump({'results': results}, f)\n",
                "            print(f\"  ðŸ’¾ Checkpoint saved at sample {idx+1}\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ Error on sample {idx}: {e}\")\n",
                "        continue\n",
                "\n",
                "print(f\"\\nâœ… Evaluation complete: {len(results)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7ï¸âƒ£ Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(f\"ðŸ“Š Results Summary (n={len(df)})\")\n",
                "print(\"=\"*60)\n",
                "print()\n",
                "\n",
                "# Key metrics\n",
                "print(\"CLINICAL CORRECTNESS:\")\n",
                "print(f\"  Phase 1 (No RAG):  {df['p1_clinical'].mean():.1%}\")\n",
                "print(f\"  Phase 2 (With RAG): {df['p2_clinical'].mean():.1%}\")\n",
                "print(f\"  RAG Lift:          +{df['rag_lift'].mean():.1%}\")\n",
                "print()\n",
                "\n",
                "print(\"FAITHFULNESS (Phase 2):\")\n",
                "print(f\"  Average: {df['faithfulness'].mean():.1%}\")\n",
                "print()\n",
                "\n",
                "print(\"CONTEXTUAL RECALL:\")\n",
                "print(f\"  Average: {df['ctx_recall'].mean():.1%}\")\n",
                "print()\n",
                "\n",
                "print(\"SCORE DISTRIBUTION:\")\n",
                "print(f\"  Perfect (100%):  {(df['p2_clinical'] >= 1.0).sum()} ({(df['p2_clinical'] >= 1.0).mean():.0%})\")\n",
                "print(f\"  Passing (â‰¥70%):  {(df['p2_clinical'] >= 0.7).sum()} ({(df['p2_clinical'] >= 0.7).mean():.0%})\")\n",
                "print(f\"  Failing (<50%):  {(df['p2_clinical'] < 0.5).sum()} ({(df['p2_clinical'] < 0.5).mean():.0%})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Plot 1: Clinical Correctness Distribution\n",
                "axes[0].hist(df['p1_clinical'], bins=10, alpha=0.5, label='No RAG', color='red')\n",
                "axes[0].hist(df['p2_clinical'], bins=10, alpha=0.5, label='With RAG', color='green')\n",
                "axes[0].set_xlabel('Clinical Correctness')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title('Score Distribution')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot 2: RAG Lift by Sample\n",
                "axes[1].bar(range(len(df)), df['rag_lift'], color=['green' if x > 0 else 'red' for x in df['rag_lift']])\n",
                "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
                "axes[1].set_xlabel('Sample')\n",
                "axes[1].set_ylabel('RAG Lift')\n",
                "axes[1].set_title('RAG Improvement by Sample')\n",
                "\n",
                "# Plot 3: Key Metrics\n",
                "metrics = ['Clinical\\nCorrectness', 'Faithfulness', 'Contextual\\nRecall']\n",
                "values = [df['p2_clinical'].mean(), df['faithfulness'].mean(), df['ctx_recall'].mean()]\n",
                "colors = ['green' if v >= 0.7 else 'orange' if v >= 0.5 else 'red' for v in values]\n",
                "axes[2].bar(metrics, values, color=colors)\n",
                "axes[2].set_ylim(0, 1)\n",
                "axes[2].set_title('Key Metrics (Phase 2)')\n",
                "for i, v in enumerate(values):\n",
                "    axes[2].text(i, v + 0.02, f'{v:.0%}', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find worst performing samples\n",
                "print(\"âŒ WORST PERFORMING SAMPLES (need attention):\")\n",
                "print(\"=\"*60)\n",
                "worst = df.nsmallest(5, 'p2_clinical')\n",
                "for _, row in worst.iterrows():\n",
                "    print(f\"Sample {row['idx']}: {row['p2_clinical']:.0%} clinical, {row['ctx_recall']:.0%} recall\")\n",
                "    print(f\"  Q: {row['question'][:80]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8ï¸âƒ£ Save Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final results\n",
                "final_results = {\n",
                "    'num_samples': len(results),\n",
                "    'config': {\n",
                "        'top_k': TOP_K,\n",
                "        'llm_model': LLM_MODEL,\n",
                "        'embed_model': EMBED_MODEL\n",
                "    },\n",
                "    'summary': {\n",
                "        'p1_clinical_mean': df['p1_clinical'].mean(),\n",
                "        'p2_clinical_mean': df['p2_clinical'].mean(),\n",
                "        'rag_lift_mean': df['rag_lift'].mean(),\n",
                "        'faithfulness_mean': df['faithfulness'].mean(),\n",
                "        'ctx_recall_mean': df['ctx_recall'].mean(),\n",
                "        'passing_rate': (df['p2_clinical'] >= 0.7).mean(),\n",
                "        'perfect_rate': (df['p2_clinical'] >= 1.0).mean()\n",
                "    },\n",
                "    'results': results\n",
                "}\n",
                "\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    json.dump(final_results, f, indent=2)\n",
                "\n",
                "print(f\"âœ… Results saved to {OUTPUT_FILE}\")\n",
                "\n",
                "# Clean up checkpoint\n",
                "if checkpoint_file.exists():\n",
                "    checkpoint_file.unlink()\n",
                "    print(f\"ðŸ—‘ï¸ Checkpoint file removed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ“Š Quick Stats\n",
                "\n",
                "Run this cell anytime to see current progress:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if results:\n",
                "    n = len(results)\n",
                "    p2_scores = [r['p2_clinical'] for r in results]\n",
                "    faith_scores = [r['faithfulness'] for r in results]\n",
                "    recall_scores = [r['ctx_recall'] for r in results]\n",
                "    \n",
                "    print(f\"\"\"ðŸ“Š CURRENT PROGRESS: {n}/{NUM_SAMPLES} samples ({n/NUM_SAMPLES*100:.0f}%)\n",
                "    \n",
                "    Clinical Correctness (With RAG): {sum(p2_scores)/n:.1%}\n",
                "    Faithfulness:                    {sum(faith_scores)/n:.1%}\n",
                "    Contextual Recall:               {sum(recall_scores)/n:.1%}\n",
                "    \n",
                "    Passing (â‰¥70%): {sum(1 for s in p2_scores if s >= 0.7)}/{n} ({sum(1 for s in p2_scores if s >= 0.7)/n*100:.0f}%)\n",
                "    Perfect (100%): {sum(1 for s in p2_scores if s >= 1.0)}/{n} ({sum(1 for s in p2_scores if s >= 1.0)/n*100:.0f}%)\n",
                "    \"\"\")\n",
                "else:\n",
                "    print(\"No results yet. Run the evaluation cells above.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}